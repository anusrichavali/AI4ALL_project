# -*- coding: utf-8 -*-
"""AI4ALL_ML_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wpiKVxGJ9nqHmis0ZGbEyGRIniZMLNfB
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas_datareader as data
import yfinance as yf
from tensorflow.keras.models import Sequential # type: ignore
from tensorflow.keras.layers import Dense, Dropout, LSTM # type: ignore

start = '2010-01-01'
end = '2019-12-31'

df = yf.download('AAPL', start=start, end=end)
print(df.head())

df = df.reset_index()

df = df.drop(['Date', 'Adj Close'], axis = 1)

plt.plot(df.Close)

#100 days moving average
ma100 = df.Close.rolling(100).mean() 

plt.figure(figsize = (12,6))
plt. plot (df.Close)
plt.plot (ma100, 'r')

#200 days moving average
ma200 = df.Close.rolling(200).mean() 

plt.figure(figsize = (12,6))
plt. plot (df.Close)
plt.plot (ma100, 'r')
plt.plot(ma200, 'g')

#split into training and testing
data_training = pd.DataFrame(df['Close'][0:int(len(df)*0.70)])
data_testing = pd.DataFrame(df['Close'][int(len(df)*0.7): int(len(df))])

# print(data_training.shape)
# print(data_testing.shape)

from sklearn.preprocessing import MinMaxScaler
#scale to treat all features equally (values will be scaled down to 0-1)
scaler = MinMaxScaler(feature_range=(0,1))

#calc the min and max values for scaling
data_training_array = scaler.fit_transform(data_training)

x_train = [] #input
y_train = [] #target output

#value of next day (y_train) will be dependent on previous 100 days (x_train)
for i in range(100, data_training_array.shape[0]):
  x_train.append(data_training_array[i-100: i])
  y_train.append(data_training_array[i, 0])

x_train, y_train = np.array(x_train), np.array(y_train)

model = Sequential() #output layer provides input to next
#add 4 layers, increase complexity at each layer
model.add(LSTM(units = 50, activation = 'relu', return_sequences= True, input_shape = (x_train.shape[1], 1)))
model.add(Dropout(0.2)) #regularization to prevent overfitting, randomly sets 20% on inputs to 0

model.add(LSTM(units = 60, activation = 'relu', return_sequences= True)) #keras infers the input shape based on output shape of prev layer
model.add(Dropout(0.3))

model.add(LSTM(units = 80, activation = 'relu', return_sequences= True))
model.add(Dropout(0.4))

model.add(LSTM(units = 120, activation = 'relu')) #only return the last output in the output sequence
model.add(Dropout(0.5))

#final prediction, fully connect all layers
model.add(Dense(units = 1))

# model.summary()

model.compile(optimizer = 'adam', loss = 'mean_squared_error')
model.fit(x_train, y_train, epochs= 50)

model.save('keras_model.h5') #save in project folder

#to predict testing data, need to get previous 100 days of 1760 index
past_100_days = data_training.tail(100)

final_df = pd.concat([past_100_days, data_testing], ignore_index=True)

#apply min max scaler to scale between 0 -1
input_data = scaler.fit_transform(final_df)

input_data.shape #1 column for closing price column

x_test = [] #input
y_test = [] #target output

#value of next day (y_train) will be dependent on previous 100 days (x_train)
for i in range(100, input_data.shape[0]):
  x_test.append(input_data[i-100: i])
  y_test.append(input_data[i, 0])

x_test, y_test = np.array(x_test), np.array(y_test)
print(x_test.shape)
print(y_test.shape)

#make predictions
y_predicted = model.predict(x_test)
y_predicted.shape

#compare test and predicted values
# scaler.scale_

#scale
scale_factor = 1 / 0.02123255
y_predicted = y_predicted * scale_factor
y_test = y_test * scale_factor

#plot graph to compare actual and predicted prices
plt.figure(figsize=(12,6))
plt.plot(y_test, 'b', label = 'Original Price')
plt.plot(y_predicted, 'r', label = 'Predicted Price')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()